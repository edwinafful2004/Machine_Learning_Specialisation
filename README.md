# ğŸ§  Linear Regression from Scratch â€” Python & NumPy

This project demonstrates a complete implementation of **Linear Regression** from scratch using only **Python and NumPy**.  
It covers fundamental concepts of **Machine Learning**, including cost function computation, gradient derivation, and gradient descent optimization.

---

## ğŸ“˜ Project Overview
Linear Regression is one of the simplest yet most powerful algorithms in Machine Learning.  
In this project, we:
- Implement the **Cost Function**:  
  J(w,b) = (1 / 2m) * Î£ (f_wb(xá¶¦) - yá¶¦)Â²
- Compute **Gradients** for parameters w and b:
  âˆ‚J/âˆ‚w = (1/m) * Î£ (f_wb(xá¶¦) - yá¶¦)xá¶¦  
  âˆ‚J/âˆ‚b = (1/m) * Î£ (f_wb(xá¶¦) - yá¶¦)
- Use **Gradient Descent** to update parameters iteratively:
  w := w - Î± âˆ‚J/âˆ‚w  
  b := b - Î± âˆ‚J/âˆ‚b

---

## ğŸ§© Features
- Compute cost and gradients manually  
- Implement gradient descent loop  
- Visualize training data and fitted line  
- Compare predicted vs actual values  
- Modular code structure with comments  

---

## ğŸ§® Example Results
| Parameter | Value |
|------------|--------|
| Initial w | 0 |
| Initial b | 0 |
| Final w | ~2.0 |
| Final b | ~0.0 |
| Final Cost | â‰ˆ 0.0 |

---

## ğŸš€ Technologies Used
- **Python 3**
- **NumPy**
- **Matplotlib**
- **Jupyter Notebook**

---

## ğŸ§  Learning Outcomes
- Understand linear regression fundamentals  
- Derive and compute the gradient from scratch  
- Implement optimization using gradient descent  
- Strengthen Python and NumPy vectorization skills  

---

## ğŸ“‚ Repository Structure
```
linear-regression-from-scratch/
â”‚
â”œâ”€â”€ Linear_Regression_From_Scratch.ipynb   # Main Jupyter Notebook
â”œâ”€â”€ public_tests.py                        # Public test functions (if used)
â”œâ”€â”€ README.md                              # Project documentation
â””â”€â”€ data/                                  # (Optional) sample data files
```

---

## ğŸ§‘â€ğŸ’» Author
**Edwin Kofi Afful**  
ğŸ“ Computer Science & Engineering Student  
ğŸ’¡ Passionate about AI, Quantum Computing, and Machine Learning  

---

## ğŸŒŸ Acknowledgments
Inspired by Andrew Ngâ€™s *Machine Learning Specialization* and DeepLearning.AI resources.  
Built as part of foundational practice for understanding gradient-based optimization.
